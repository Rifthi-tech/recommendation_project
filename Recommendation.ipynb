{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "QNg5kHPG-mK8",
        "1M540epC-g6z",
        "wqQiBTBw_G-5",
        "kO793eK7CPpH",
        "irVcWtXcDHDh",
        "cyKZU0QaD_XD"
      ],
      "gpuType": "T4",
      "mount_file_id": "1bGdMZBlEOpkmc5vkqduDdEfyfnjndl8N",
      "authorship_tag": "ABX9TyNx5zqOnRj/iRDQvOrmKOu+",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Rifthi-tech/recommendation_project/blob/main/Recommendation.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Setting Up the Environment"
      ],
      "metadata": {
        "id": "QNg5kHPG-mK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Core data processing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "\n",
        "# Machine learning\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "# Utilities\n",
        "from datetime import datetime\n",
        "import pickle\n",
        "import os"
      ],
      "metadata": {
        "id": "YJ9ZV7_R-sYM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Import data"
      ],
      "metadata": {
        "id": "1M540epC-g6z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv(\"/content/drive/MyDrive/Recommendation Project/sample.csv\")  # Replace with your actual file path\n",
        "print(df.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f3enqo9M-EnS",
        "outputId": "ad0af864-f3d2-4610-ba69-198a286f12c3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "                            uniq_id  \\\n",
            "0  c2d766ca982eca8304150849735ffef9   \n",
            "1  7f7036a6d550aaa89d34c77bd39a5e48   \n",
            "2  f449ec65dcbc041b6ae5e6a32717d01b   \n",
            "3  0973b37acd0c664e3de26e97e5571454   \n",
            "4  bc940ea42ee6bef5ac7cea3fb5cfbee7   \n",
            "\n",
            "                                         product_url  \\\n",
            "0  http://www.flipkart.com/alisha-solid-women-s-c...   \n",
            "1  http://www.flipkart.com/fabhomedecor-fabric-do...   \n",
            "2  http://www.flipkart.com/aw-bellies/p/itmeh4grg...   \n",
            "3  http://www.flipkart.com/alisha-solid-women-s-c...   \n",
            "4  http://www.flipkart.com/sicons-all-purpose-arn...   \n",
            "\n",
            "                            product_name  \\\n",
            "0    Alisha Solid Women's Cycling Shorts   \n",
            "1    FabHomeDecor Fabric Double Sofa Bed   \n",
            "2                             AW Bellies   \n",
            "3                            product_url   \n",
            "4  Sicons All Purpose Arnica Dog Shampoo   \n",
            "\n",
            "                               product_category_tree  Unnamed: 4  \\\n",
            "0  [\"Clothing >> Women's Clothing >> Lingerie, Sl...         NaN   \n",
            "1  [\"Furniture >> Living Room Furniture >> Sofa B...         NaN   \n",
            "2  [\"Footwear >> Women's Footwear >> Ballerinas >...         NaN   \n",
            "3  [\"Clothing >> Women's Clothing >> Lingerie, Sl...         NaN   \n",
            "4  [\"Pet Supplies >> Grooming >> Skin & Coat Care...         NaN   \n",
            "\n",
            "   retail_price                                              image  \\\n",
            "0         999.0  [\"http://img5a.flixcart.com/image/short/u/4/a/...   \n",
            "1       32157.0  [\"http://img6a.flixcart.com/image/sofa-bed/j/f...   \n",
            "2         999.0  [\"http://img5a.flixcart.com/image/shoe/7/z/z/r...   \n",
            "3         699.0  [\"http://img5a.flixcart.com/image/short/6/2/h/...   \n",
            "4         220.0  [\"http://img5a.flixcart.com/image/pet-shampoo/...   \n",
            "\n",
            "                                         description       product_rating  \\\n",
            "0  Key Features of Alisha Solid Women's Cycling S...  No rating available   \n",
            "1  FabHomeDecor Fabric Double Sofa Bed (Finish Co...  No rating available   \n",
            "2  Key Features of AW Bellies Sandals Wedges Heel...  No rating available   \n",
            "3  Key Features of Alisha Solid Women's Cycling S...  No rating available   \n",
            "4  Specifications of Sicons All Purpose Arnica Do...  No rating available   \n",
            "\n",
            "          brand                             product_specifications  \n",
            "0        Alisha  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
            "1  FabHomeDecor  {\"product_specification\"=>[{\"key\"=>\"Installati...  \n",
            "2            AW  {\"product_specification\"=>[{\"key\"=>\"Ideal For\"...  \n",
            "3        Alisha  {\"product_specification\"=>[{\"key\"=>\"Number of ...  \n",
            "4        Sicons  {\"product_specification\"=>[{\"key\"=>\"Pet Type\",...  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Cleaning"
      ],
      "metadata": {
        "id": "wqQiBTBw_G-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generate_sample_data():\n",
        "    \"\"\"Generate sample product, rating, and purchase data\"\"\"\n",
        "    # Sample products\n",
        "    products = pd.DataFrame({\n",
        "        'product_id': [101, 102, 103, 104, 105],\n",
        "        'name': ['Laptop', 'Smartphone', 'Headphones', 'Keyboard', 'Mouse'],\n",
        "        'price': [999.99, 699.99, '149.99', 79.99, np.nan],  # Intentional issues\n",
        "        'category': ['Electronics', 'Electronics', None, 'Accessories', 'Accessories'],\n",
        "        'description': ['High-performance', '', 'Noise-cancelling', None, 'Wireless']\n",
        "    })\n",
        "\n",
        "    # Sample ratings (with some invalid entries)\n",
        "    ratings = pd.DataFrame({\n",
        "        'user_id': ['U1', 'U2', 'U3', 'U4', 'U5'],\n",
        "        'product_id': [101, 102, 999, 104, 105],\n",
        "        'rating': [5, 4, 0, 6, 3],  # 0 and 6 are invalid\n",
        "        'timestamp': ['2023-01-01', '2023-01-02', '2023-01-03', 'invalid_date', '2023-01-05']\n",
        "    })\n",
        "\n",
        "    # Sample purchases\n",
        "    purchases = pd.DataFrame({\n",
        "        'purchase_id': [1, 2, 3, 4, 5],\n",
        "        'user_id': ['U1', 'U2', 'U3', 'U4', 'U5'],\n",
        "        'product_id': [101, 102, 103, 104, 105],\n",
        "        'purchase_date': ['2023-01-01 10:00', '2023-01-02 11:30', 'invalid', '2023-01-04', None]\n",
        "    })\n",
        "\n",
        "    return products, ratings, purchases\n",
        "\n",
        "def clean_data(products, ratings, purchases):\n",
        "\n",
        "\n",
        "\n",
        "    products.fillna({\n",
        "        'description': 'No description',\n",
        "        'category': 'uncategorized'\n",
        "    }, inplace=True)\n",
        "\n",
        "    products['price'] = pd.to_numeric(products['price'], errors='coerce')\n",
        "    products['price'].fillna(products['price'].mean(), inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    ratings = ratings[ratings['rating'].between(1, 5)]  # Keep only valid ratings\n",
        "    ratings['timestamp'] = pd.to_datetime(ratings['timestamp'], errors='coerce')\n",
        "    ratings.dropna(subset=['timestamp'], inplace=True)\n",
        "\n",
        "\n",
        "\n",
        "    purchases['purchase_date'] = pd.to_datetime(purchases['purchase_date'], errors='coerce')\n",
        "    purchases.dropna(subset=['purchase_date'], inplace=True)\n",
        "\n",
        "    print(\"\\n Data cleaning completed successfully!\")\n",
        "    return products, ratings, purchases\n",
        "\n",
        "def main():\n",
        "\n",
        "\n",
        "    # 1. Generate sample data\n",
        "\n",
        "    products, ratings, purchases = generate_sample_data()\n",
        "\n",
        "    # 2. Clean data\n",
        "    clean_products, clean_ratings, clean_purchases = clean_data(products.copy(),\n",
        "                                                              ratings.copy(),\n",
        "                                                              purchases.copy())\n",
        "\n",
        "    # 3. Show ALL results\n",
        "\n",
        "\n",
        "    print(\"\\n CLEANED PRODUCTS (ALL):\")\n",
        "    print(clean_products.to_markdown(tablefmt=\"grid\", numalign=\"center\"))\n",
        "\n",
        "    print(\"\\n CLEANED RATINGS (ALL):\")\n",
        "    print(clean_ratings.to_markdown(tablefmt=\"grid\", numalign=\"center\"))\n",
        "\n",
        "    print(\"\\n CLEANED PURCHASES (ALL):\")\n",
        "    print(clean_purchases.to_markdown(tablefmt=\"grid\", numalign=\"center\"))\n",
        "\n",
        "\n",
        "    print(\" CLEANING STATS:\")\n",
        "    print(f\"Products: {len(clean_products)} records (Original: {len(products)}) - All kept with fixes\")\n",
        "    print(f\"Ratings: {len(clean_ratings)} valid records (Original: {len(ratings)}) - {len(ratings)-len(clean_ratings)} removed\")\n",
        "    print(f\"Purchases: {len(clean_purchases)} valid records (Original: {len(purchases)}) - {len(purchases)-len(clean_purchases)} removed\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    print(\"\\n SUCCESS: All data has been cleaned and is now ready for analysis!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kogu4xXR_KgW",
        "outputId": "458d5fa8-25e1-4644-8249-f69fa7002e3b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " Data cleaning completed successfully!\n",
            "\n",
            " CLEANED PRODUCTS (ALL):\n",
            "+----+--------------+------------+---------+---------------+------------------+\n",
            "|    |  product_id  | name       |  price  | category      | description      |\n",
            "+====+==============+============+=========+===============+==================+\n",
            "| 0  |     101      | Laptop     | 999.99  | Electronics   | High-performance |\n",
            "+----+--------------+------------+---------+---------------+------------------+\n",
            "| 1  |     102      | Smartphone | 699.99  | Electronics   |                  |\n",
            "+----+--------------+------------+---------+---------------+------------------+\n",
            "| 2  |     103      | Headphones | 149.99  | uncategorized | Noise-cancelling |\n",
            "+----+--------------+------------+---------+---------------+------------------+\n",
            "| 3  |     104      | Keyboard   |  79.99  | Accessories   | No description   |\n",
            "+----+--------------+------------+---------+---------------+------------------+\n",
            "| 4  |     105      | Mouse      | 482.49  | Accessories   | Wireless         |\n",
            "+----+--------------+------------+---------+---------------+------------------+\n",
            "\n",
            " CLEANED RATINGS (ALL):\n",
            "+----+-----------+--------------+----------+---------------------+\n",
            "|    | user_id   |  product_id  |  rating  | timestamp           |\n",
            "+====+===========+==============+==========+=====================+\n",
            "| 0  | U1        |     101      |    5     | 2023-01-01 00:00:00 |\n",
            "+----+-----------+--------------+----------+---------------------+\n",
            "| 1  | U2        |     102      |    4     | 2023-01-02 00:00:00 |\n",
            "+----+-----------+--------------+----------+---------------------+\n",
            "| 4  | U5        |     105      |    3     | 2023-01-05 00:00:00 |\n",
            "+----+-----------+--------------+----------+---------------------+\n",
            "\n",
            " CLEANED PURCHASES (ALL):\n",
            "+----+---------------+-----------+--------------+---------------------+\n",
            "|    |  purchase_id  | user_id   |  product_id  | purchase_date       |\n",
            "+====+===============+===========+==============+=====================+\n",
            "| 0  |       1       | U1        |     101      | 2023-01-01 10:00:00 |\n",
            "+----+---------------+-----------+--------------+---------------------+\n",
            "| 1  |       2       | U2        |     102      | 2023-01-02 11:30:00 |\n",
            "+----+---------------+-----------+--------------+---------------------+\n",
            " CLEANING STATS:\n",
            "Products: 5 records (Original: 5) - All kept with fixes\n",
            "Ratings: 3 valid records (Original: 5) - 2 removed\n",
            "Purchases: 2 valid records (Original: 5) - 3 removed\n",
            "==================================================\n",
            "\n",
            " SUCCESS: All data has been cleaned and is now ready for analysis!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-8-248f252e47b7>:40: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
            "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
            "\n",
            "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
            "\n",
            "\n",
            "  products['price'].fillna(products['price'].mean(), inplace=True)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#4.Feature Extraction"
      ],
      "metadata": {
        "id": "kO793eK7CPpH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def feature_extraction(products, ratings, purchases):\n",
        "    try:\n",
        "        print(\"Starting feature extraction process...\\n\")\n",
        "\n",
        "        # Product features\n",
        "        print(\"Step 1/5: Extracting product features using TF-IDF...\")\n",
        "        tfidf = TfidfVectorizer(stop_words='english')\n",
        "        product_features = tfidf.fit_transform(products['description'] + \" \" + products['category'])\n",
        "        print(f\" Success! Product features matrix shape: {product_features.shape}\\n\")\n",
        "\n",
        "        # User features from ratings\n",
        "        print(\"Step 2/5: Extracting user features from ratings data...\")\n",
        "        user_ratings = ratings.groupby('user_id').agg({\n",
        "            'rating': ['mean', 'count'],\n",
        "            'product_id': 'nunique'\n",
        "        }).reset_index()\n",
        "        user_ratings.columns = ['user_id', 'avg_rating', 'rating_count', 'unique_products_rated']\n",
        "        print(f\" Success! Generated {len(user_ratings)} user rating records\")\n",
        "        print(\"First 3 user rating features:\")\n",
        "        print(user_ratings.head(3).to_string(), \"\\n\")\n",
        "\n",
        "        # User features from purchases\n",
        "        print(\"Step 3/5: Extracting user features from purchases data...\")\n",
        "        user_purchases = purchases.groupby('user_id').agg({\n",
        "            'product_id': ['count', 'nunique'],\n",
        "            'purchase_date': 'max'\n",
        "        }).reset_index()\n",
        "        user_purchases.columns = ['user_id', 'total_purchases', 'unique_products_purchased', 'last_purchase_date']\n",
        "        print(f\" Success! Generated {len(user_purchases)} user purchase records\")\n",
        "        print(\"First 3 user purchase features:\")\n",
        "        print(user_purchases.head(3).to_string(), \"\\n\")\n",
        "\n",
        "        # Merge user features\n",
        "        print(\"Step 4/5: Merging all user features...\")\n",
        "        user_features = pd.merge(user_ratings, user_purchases, on='user_id', how='outer')\n",
        "        user_features.fillna(0, inplace=True)\n",
        "        print(f\" Success! Merged {len(user_features)} user feature records\")\n",
        "        print(\"First 3 merged user features:\")\n",
        "        print(user_features.head(3).to_string(), \"\\n\")\n",
        "\n",
        "        # Normalize features\n",
        "        print(\"Step 5/5: Normalizing user features...\")\n",
        "        scaler = MinMaxScaler()\n",
        "        cols_to_normalize = ['avg_rating', 'rating_count', 'unique_products_rated',\n",
        "                            'total_purchases', 'unique_products_purchased']\n",
        "        user_features[cols_to_normalize] = scaler.fit_transform(user_features[cols_to_normalize])\n",
        "        print(\" Successfully normalized user features!\")\n",
        "        print(\"First 3 normalized user features:\")\n",
        "        print(user_features.head(3).to_string(), \"\\n\")\n",
        "\n",
        "        print(\"\\n Feature extraction completed successfully! \")\n",
        "        print(f\"Final product features shape: {product_features.shape}\")\n",
        "        print(f\"Final user features shape: {user_features.shape}\\n\")\n",
        "\n",
        "        return product_features, user_features, tfidf, scaler\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Error in feature extraction: {str(e)}\")\n",
        "        raise\n",
        "\n",
        "# Example usage:\n",
        "if __name__ == \"__main__\":\n",
        "    # Create sample data to test\n",
        "    import pandas as pd\n",
        "    from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "    from sklearn.preprocessing import MinMaxScaler\n",
        "\n",
        "    products = pd.DataFrame({\n",
        "        'description': ['phone', 'laptop', 'tablet'],\n",
        "        'category': ['electronics', 'electronics', 'electronics']\n",
        "    })\n",
        "\n",
        "    ratings = pd.DataFrame({\n",
        "        'user_id': [1, 1, 2, 2, 3],\n",
        "        'product_id': [1, 2, 1, 3, 2],\n",
        "        'rating': [5, 4, 3, 5, 4]\n",
        "    })\n",
        "\n",
        "    purchases = pd.DataFrame({\n",
        "        'user_id': [1, 1, 2, 3, 3],\n",
        "        'product_id': [1, 2, 1, 2, 3],\n",
        "        'purchase_date': pd.to_datetime(['2023-01-01', '2023-02-01', '2023-01-15', '2023-03-01', '2023-03-15'])\n",
        "    })\n",
        "\n",
        "    # Call the function\n",
        "    product_features, user_features, tfidf, scaler = feature_extraction(products, ratings, purchases)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8zjiyLCJCU5N",
        "outputId": "137ffdfb-210e-436f-9633-a9f84017c0c0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Starting feature extraction process...\n",
            "\n",
            "Step 1/5: Extracting product features using TF-IDF...\n",
            " Success! Product features matrix shape: (3, 4)\n",
            "\n",
            "Step 2/5: Extracting user features from ratings data...\n",
            " Success! Generated 3 user rating records\n",
            "First 3 user rating features:\n",
            "   user_id  avg_rating  rating_count  unique_products_rated\n",
            "0        1         4.5             2                      2\n",
            "1        2         4.0             2                      2\n",
            "2        3         4.0             1                      1 \n",
            "\n",
            "Step 3/5: Extracting user features from purchases data...\n",
            " Success! Generated 3 user purchase records\n",
            "First 3 user purchase features:\n",
            "   user_id  total_purchases  unique_products_purchased last_purchase_date\n",
            "0        1                2                          2         2023-02-01\n",
            "1        2                1                          1         2023-01-15\n",
            "2        3                2                          2         2023-03-15 \n",
            "\n",
            "Step 4/5: Merging all user features...\n",
            " Success! Merged 3 user feature records\n",
            "First 3 merged user features:\n",
            "   user_id  avg_rating  rating_count  unique_products_rated  total_purchases  unique_products_purchased last_purchase_date\n",
            "0        1         4.5             2                      2                2                          2         2023-02-01\n",
            "1        2         4.0             2                      2                1                          1         2023-01-15\n",
            "2        3         4.0             1                      1                2                          2         2023-03-15 \n",
            "\n",
            "Step 5/5: Normalizing user features...\n",
            " Successfully normalized user features!\n",
            "First 3 normalized user features:\n",
            "   user_id  avg_rating  rating_count  unique_products_rated  total_purchases  unique_products_purchased last_purchase_date\n",
            "0        1         1.0           1.0                    1.0              1.0                        1.0         2023-02-01\n",
            "1        2         0.0           1.0                    1.0              0.0                        0.0         2023-01-15\n",
            "2        3         0.0           0.0                    0.0              1.0                        1.0         2023-03-15 \n",
            "\n",
            "\n",
            " Feature extraction completed successfully! \n",
            "Final product features shape: (3, 4)\n",
            "Final user features shape: (3, 7)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Model Training"
      ],
      "metadata": {
        "id": "irVcWtXcDHDh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def display_all_model_outputs(results):\n",
        "    \"\"\"Displays **all** trained model outputs with clear success messages and formatted data\"\"\"\n",
        "\n",
        "    try:\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\" STARTING MODEL OUTPUT DISPLAY\".center(50))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "        # 1. User similarity matrix\n",
        "        print(\"\\n STEP 1/4: Displaying User Similarity Matrix\")\n",
        "        user_sim_df = pd.DataFrame(results['user_similarity'])\n",
        "        print(f\" Success! User similarity matrix shape: {user_sim_df.shape}\")\n",
        "        print(\"Top 5x5 portion of User Similarity Matrix:\")\n",
        "        print(user_sim_df.iloc[:5, :5].to_markdown(tablefmt=\"grid\", floatfmt=\".2f\"))\n",
        "\n",
        "        # 2. Product similarity matrix\n",
        "        print(\"\\n STEP 2/4: Displaying Product Similarity Matrix\")\n",
        "        product_sim_df = pd.DataFrame(results['product_similarity'])\n",
        "        print(f\" Success! Product similarity matrix shape: {product_sim_df.shape}\")\n",
        "        print(\"Top 5x5 portion of Product Similarity Matrix:\")\n",
        "        print(product_sim_df.iloc[:5, :5].to_markdown(tablefmt=\"grid\", floatfmt=\".2f\"))\n",
        "\n",
        "        # 3. Purchase matrix\n",
        "        print(\"\\n STEP 3/4: Displaying Purchase Matrix\")\n",
        "        print(f\" Success! Purchase matrix shape: {results['purchase_matrix'].shape}\")\n",
        "        print(\"Sample of Purchase Matrix (first 5 rows):\")\n",
        "        print(results['purchase_matrix'].head().to_markdown(tablefmt=\"grid\"))\n",
        "\n",
        "        # 4. Hybrid model weights\n",
        "        print(\"\\n STEP 4/4: Displaying Hybrid Model Weights\")\n",
        "        weights_df = pd.DataFrame.from_dict(results['hybrid_weights'], orient='index', columns=['Weight'])\n",
        "        print(\" Success! Retrieved hybrid model weights\")\n",
        "        print(\"Hybrid Model Weights:\")\n",
        "        print(weights_df.to_markdown(tablefmt=\"grid\", floatfmt=\".2f\"))\n",
        "\n",
        "        print(\"\\n\" + \"=\"*50)\n",
        "        print(\" ALL MODEL OUTPUTS DISPLAYED SUCCESSFULLY!\".center(50))\n",
        "        print(\"=\"*50)\n",
        "\n",
        "    except KeyError as e:\n",
        "        print(f\"\\n Error: Missing expected key in results - {str(e)}\")\n",
        "    except Exception as e:\n",
        "        print(f\"\\n Unexpected error displaying outputs: {str(e)}\")\n",
        "\n",
        "# Example usage with test data\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "    # Create sample model results\n",
        "    model_results = {\n",
        "        'user_similarity': np.random.rand(10, 10),\n",
        "        'product_similarity': np.random.rand(15, 15),\n",
        "        'purchase_matrix': pd.DataFrame(np.random.randint(0, 2, (10, 15))),\n",
        "        'hybrid_weights': {\n",
        "            'content_weight': 0.4,\n",
        "            'collab_weight': 0.3,\n",
        "            'popularity_weight': 0.2,\n",
        "            'recency_weight': 0.1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    # Call the function\n",
        "    display_all_model_outputs(model_results)"
      ],
      "metadata": {
        "id": "6fapTwe8DLSy",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c43deeba-02a4-4289-b441-bacada3248c9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "           STARTING MODEL OUTPUT DISPLAY          \n",
            "==================================================\n",
            "\n",
            " STEP 1/4: Displaying User Similarity Matrix\n",
            " Success! User similarity matrix shape: (10, 10)\n",
            "Top 5x5 portion of User Similarity Matrix:\n",
            "+----+------+------+------+------+------+\n",
            "|    |    0 |    1 |    2 |    3 |    4 |\n",
            "+====+======+======+======+======+======+\n",
            "|  0 | 0.17 | 0.14 | 0.72 | 0.44 | 0.78 |\n",
            "+----+------+------+------+------+------+\n",
            "|  1 | 0.85 | 1.00 | 0.49 | 1.00 | 0.57 |\n",
            "+----+------+------+------+------+------+\n",
            "|  2 | 0.22 | 0.31 | 0.09 | 0.69 | 0.20 |\n",
            "+----+------+------+------+------+------+\n",
            "|  3 | 0.59 | 0.43 | 0.80 | 0.99 | 0.40 |\n",
            "+----+------+------+------+------+------+\n",
            "|  4 | 0.42 | 0.62 | 0.12 | 0.42 | 0.01 |\n",
            "+----+------+------+------+------+------+\n",
            "\n",
            " STEP 2/4: Displaying Product Similarity Matrix\n",
            " Success! Product similarity matrix shape: (15, 15)\n",
            "Top 5x5 portion of Product Similarity Matrix:\n",
            "+----+------+------+------+------+------+\n",
            "|    |    0 |    1 |    2 |    3 |    4 |\n",
            "+====+======+======+======+======+======+\n",
            "|  0 | 0.95 | 0.33 | 0.09 | 0.02 | 0.82 |\n",
            "+----+------+------+------+------+------+\n",
            "|  1 | 0.90 | 0.15 | 0.23 | 0.40 | 0.39 |\n",
            "+----+------+------+------+------+------+\n",
            "|  2 | 0.44 | 0.54 | 0.93 | 0.39 | 0.75 |\n",
            "+----+------+------+------+------+------+\n",
            "|  3 | 0.63 | 0.99 | 0.94 | 0.59 | 0.68 |\n",
            "+----+------+------+------+------+------+\n",
            "|  4 | 0.52 | 0.49 | 0.77 | 0.94 | 0.96 |\n",
            "+----+------+------+------+------+------+\n",
            "\n",
            " STEP 3/4: Displaying Purchase Matrix\n",
            " Success! Purchase matrix shape: (10, 15)\n",
            "Sample of Purchase Matrix (first 5 rows):\n",
            "+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+\n",
            "|    |   0 |   1 |   2 |   3 |   4 |   5 |   6 |   7 |   8 |   9 |   10 |   11 |   12 |   13 |   14 |\n",
            "+====+=====+=====+=====+=====+=====+=====+=====+=====+=====+=====+======+======+======+======+======+\n",
            "|  0 |   1 |   0 |   0 |   1 |   1 |   1 |   1 |   1 |   0 |   1 |    0 |    0 |    0 |    0 |    0 |\n",
            "+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+\n",
            "|  1 |   1 |   1 |   0 |   1 |   0 |   1 |   0 |   0 |   0 |   0 |    1 |    1 |    0 |    0 |    0 |\n",
            "+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+\n",
            "|  2 |   0 |   1 |   1 |   1 |   1 |   0 |   1 |   1 |   0 |   1 |    1 |    1 |    0 |    0 |    1 |\n",
            "+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+\n",
            "|  3 |   1 |   0 |   1 |   1 |   0 |   1 |   0 |   0 |   1 |   1 |    0 |    0 |    0 |    0 |    0 |\n",
            "+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+\n",
            "|  4 |   1 |   0 |   0 |   1 |   1 |   1 |   1 |   0 |   1 |   1 |    0 |    1 |    1 |    1 |    0 |\n",
            "+----+-----+-----+-----+-----+-----+-----+-----+-----+-----+-----+------+------+------+------+------+\n",
            "\n",
            " STEP 4/4: Displaying Hybrid Model Weights\n",
            " Success! Retrieved hybrid model weights\n",
            "Hybrid Model Weights:\n",
            "+-------------------+----------+\n",
            "|                   |   Weight |\n",
            "+===================+==========+\n",
            "| content_weight    |     0.40 |\n",
            "+-------------------+----------+\n",
            "| collab_weight     |     0.30 |\n",
            "+-------------------+----------+\n",
            "| popularity_weight |     0.20 |\n",
            "+-------------------+----------+\n",
            "| recency_weight    |     0.10 |\n",
            "+-------------------+----------+\n",
            "\n",
            "==================================================\n",
            "     ALL MODEL OUTPUTS DISPLAYED SUCCESSFULLY!    \n",
            "==================================================\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. Recommendation Functions"
      ],
      "metadata": {
        "id": "cyKZU0QaD_XD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def collaborative_recommendation(user_id, model_data):\n",
        "    \"\"\"Generate collaborative filtering recommendations with detailed output\"\"\"\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find user index\n",
        "        print(f\"\\nSTEP 1: Locating user {user_id} in dataset...\")\n",
        "        user_idx = np.where(model_data['user_ids'] == user_id)[0][0]\n",
        "        print(f\" Found user at index {user_idx}\")\n",
        "\n",
        "        # Step 2: Calculate similarity scores\n",
        "        print(\"\\nSTEP 2: Calculating user similarity scores...\")\n",
        "        user_sim_scores = model_data['user_similarity'][user_idx]\n",
        "        print(f\" Calculated similarity scores for {len(user_sim_scores)} users\")\n",
        "\n",
        "        # Step 3: Get similar users\n",
        "        print(\"\\nSTEP 3: Identifying top similar users...\")\n",
        "        similar_users = np.argsort(user_sim_scores)[-10:-1]\n",
        "        print(f\" Top similar users found: {similar_users}\")\n",
        "\n",
        "        # Step 4: Get products from similar users\n",
        "        print(\"\\nSTEP 4: Analyzing products from similar users...\")\n",
        "        similar_users_products = model_data['user_product_matrix'].iloc[similar_users]\n",
        "        recommended_products = similar_users_products.mean(axis=0).sort_values(ascending=False)\n",
        "        print(f\" Identified {len(recommended_products)} potential products\")\n",
        "\n",
        "        # Step 5: Filter already purchased products\n",
        "        print(\"\\nSTEP 5: Filtering out already purchased products...\")\n",
        "        user_products = set(model_data['user_product_matrix'].loc[user_id]\n",
        "                          [model_data['user_product_matrix'].loc[user_id] > 0].index)\n",
        "        recommended_products = recommended_products[~recommended_products.index.isin(user_products)]\n",
        "        print(f\" Filtered to {len(recommended_products)} new recommendations\")\n",
        "\n",
        "        # Final output\n",
        "\n",
        "        print(f\" COLLABORATIVE RECOMMENDATIONS FOR USER {user_id}\".center(60))\n",
        "\n",
        "        print(f\"\\nTop 10 Recommendations (of {len(recommended_products)} total):\")\n",
        "        print(recommended_products.head(10).to_frame(name='Recommendation Score').to_markdown(tablefmt=\"grid\", floatfmt=\".3f\"))\n",
        "\n",
        "        return recommended_products.index.tolist()\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n ERROR in Collaborative Recommendation:\", str(e))\n",
        "        return []\n",
        "\n",
        "def content_based_recommendation(product_id, model_data):\n",
        "    \"\"\"Generate content-based recommendations with detailed output\"\"\"\n",
        "\n",
        "    print(\" STARTING CONTENT-BASED RECOMMENDATION\".center(60))\n",
        "\n",
        "\n",
        "    try:\n",
        "        # Step 1: Find product index\n",
        "        print(f\"\\nSTEP 1: Locating product {product_id} in dataset...\")\n",
        "        product_idx = np.where(model_data['product_ids'] == product_id)[0][0]\n",
        "        print(f\" Found product at index {product_idx}\")\n",
        "\n",
        "        # Step 2: Get similarity scores\n",
        "        print(\"\\nSTEP 2: Calculating product similarity scores...\")\n",
        "        sim_scores = list(enumerate(model_data['product_similarity'][product_idx]))\n",
        "        sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n",
        "        sim_scores = sim_scores[1:11]  # Get top 10 similar products\n",
        "        print(f\" Calculated similarity scores for {len(sim_scores)} products\")\n",
        "\n",
        "        # Step 3: Prepare recommendations\n",
        "        print(\"\\nSTEP 3: Preparing recommendations...\")\n",
        "        similar_products = [(model_data['product_ids'][i], score) for i, score in sim_scores]\n",
        "\n",
        "        # Final output\n",
        "\n",
        "        print(f\" CONTENT-BASED RECOMMENDATIONS FOR PRODUCT {product_id}\".center(60))\n",
        "\n",
        "        print(\"\\nTop 10 Similar Products:\")\n",
        "        print(pd.DataFrame(similar_products, columns=['Product ID', 'Similarity Score'])\n",
        "                  .to_markdown(tablefmt=\"grid\", floatfmt=\".3f\"))\n",
        "\n",
        "        return [product for product, score in similar_products]\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n ERROR in Content-Based Recommendation:\", str(e))\n",
        "        return []\n",
        "\n",
        "def hybrid_recommendation(user_id, product_id, model_data):\n",
        "    \"\"\"Generate hybrid recommendations with detailed output\"\"\"\n",
        "\n",
        "    print(\" STARTING HYBRID RECOMMENDATION\".center(60))\n",
        "    print(f\"User ID: {user_id} | Product ID: {product_id}\")\n",
        "\n",
        "    try:\n",
        "        # Step 1: Get collaborative recommendations\n",
        "        print(\"\\nPHASE 1: Running Collaborative Filtering...\")\n",
        "        collab_rec = collaborative_recommendation(user_id, model_data)\n",
        "        print(f\" Got {len(collab_rec)} collaborative recommendations\")\n",
        "\n",
        "        # Step 2: Get content-based recommendations\n",
        "        print(\"\\nPHASE 2: Running Content-Based Filtering...\")\n",
        "        content_rec = content_based_recommendation(product_id, model_data)\n",
        "        print(f\" Got {len(content_rec)} content-based recommendations\")\n",
        "\n",
        "        # Step 3: Combine recommendations\n",
        "        print(\"\\nPHASE 3: Combining recommendations with weights...\")\n",
        "        print(f\"Weights: Collaborative={model_data['hybrid_weights']['collaborative']}, \"\n",
        "              f\"Content={model_data['hybrid_weights']['content']}\")\n",
        "\n",
        "        recommendations = {}\n",
        "        for product in collab_rec:\n",
        "            recommendations[product] = model_data['hybrid_weights']['collaborative']\n",
        "\n",
        "        for product in content_rec:\n",
        "            recommendations[product] = recommendations.get(product, 0) + model_data['hybrid_weights']['content']\n",
        "\n",
        "        # Step 4: Sort and finalize\n",
        "        sorted_recommendations = sorted(recommendations.items(), key=lambda x: x[1], reverse=True)\n",
        "        final_recommendations = [product for product, score in sorted_recommendations]\n",
        "\n",
        "        # Final output\n",
        "\n",
        "        print(f\" HYBRID RECOMMENDATIONS FOR USER {user_id}\".center(60))\n",
        "\n",
        "        print(\"\\nTop 10 Combined Recommendations:\")\n",
        "        print(pd.DataFrame(sorted_recommendations[:10], columns=['Product ID', 'Combined Score'])\n",
        "                  .to_markdown(tablefmt=\"grid\", floatfmt=\".3f\"))\n",
        "        print(f\"\\nTotal unique recommendations: {len(final_recommendations)}\")\n",
        "\n",
        "        return final_recommendations\n",
        "\n",
        "    except Exception as e:\n",
        "        print(\"\\n ERROR in Hybrid Recommendation:\", str(e))\n",
        "        return []\n",
        "\n",
        "# Test execution\n",
        "if __name__ == \"__main__\":\n",
        "    import pandas as pd\n",
        "    import numpy as np\n",
        "\n",
        "\n",
        "\n",
        "    model_data = {\n",
        "        'user_ids': np.array([1, 2, 3, 4, 5]),\n",
        "        'product_ids': np.array(['P1', 'P2', 'P3', 'P4', 'P5']),\n",
        "        'user_similarity': np.random.rand(5, 5),\n",
        "        'product_similarity': np.random.rand(5, 5),\n",
        "        'user_product_matrix': pd.DataFrame(np.random.randint(0, 2, (5, 5))),\n",
        "        'hybrid_weights': {\n",
        "            'collaborative': 0.6,\n",
        "            'content': 0.4\n",
        "        }\n",
        "  }\n",
        "\n",
        "    # Set test parameters\n",
        "    test_user = 3\n",
        "    test_product = \"P3\"\n",
        "\n",
        "    # Run hybrid recommendation\n",
        "\n",
        "    print(\" TESTING HYBRID RECOMMENDATION SYSTEM\".center(60))\n",
        "\n",
        "\n",
        "    final_recommendations = hybrid_recommendation(test_user, test_product, model_data)\n",
        "\n",
        "\n",
        "    print(f\" FINAL RECOMMENDATIONS FOR USER {test_user}\".center(60))\n",
        "    print(f\"\\nReturned {len(final_recommendations)} recommendations:\")\n",
        "    print(final_recommendations)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zapbEcvqED3e",
        "outputId": "ded41099-dd95-4d6b-a3b2-f989208c6911"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "            TESTING HYBRID RECOMMENDATION SYSTEM            \n",
            "               STARTING HYBRID RECOMMENDATION               \n",
            "User ID: 3 | Product ID: P3\n",
            "\n",
            "PHASE 1: Running Collaborative Filtering...\n",
            "\n",
            "STEP 1: Locating user 3 in dataset...\n",
            " Found user at index 2\n",
            "\n",
            "STEP 2: Calculating user similarity scores...\n",
            " Calculated similarity scores for 5 users\n",
            "\n",
            "STEP 3: Identifying top similar users...\n",
            " Top similar users found: [3 0 2 1]\n",
            "\n",
            "STEP 4: Analyzing products from similar users...\n",
            " Identified 5 potential products\n",
            "\n",
            "STEP 5: Filtering out already purchased products...\n",
            " Filtered to 2 new recommendations\n",
            "          COLLABORATIVE RECOMMENDATIONS FOR USER 3          \n",
            "\n",
            "Top 10 Recommendations (of 2 total):\n",
            "+----+------------------------+\n",
            "|    |   Recommendation Score |\n",
            "+====+========================+\n",
            "|  2 |                  0.500 |\n",
            "+----+------------------------+\n",
            "|  0 |                  0.250 |\n",
            "+----+------------------------+\n",
            " Got 2 collaborative recommendations\n",
            "\n",
            "PHASE 2: Running Content-Based Filtering...\n",
            "            STARTING CONTENT-BASED RECOMMENDATION           \n",
            "\n",
            "STEP 1: Locating product P3 in dataset...\n",
            " Found product at index 2\n",
            "\n",
            "STEP 2: Calculating product similarity scores...\n",
            " Calculated similarity scores for 4 products\n",
            "\n",
            "STEP 3: Preparing recommendations...\n",
            "        CONTENT-BASED RECOMMENDATIONS FOR PRODUCT P3        \n",
            "\n",
            "Top 10 Similar Products:\n",
            "+----+--------------+--------------------+\n",
            "|    | Product ID   |   Similarity Score |\n",
            "+====+==============+====================+\n",
            "|  0 | P3           |              0.577 |\n",
            "+----+--------------+--------------------+\n",
            "|  1 | P4           |              0.542 |\n",
            "+----+--------------+--------------------+\n",
            "|  2 | P1           |              0.231 |\n",
            "+----+--------------+--------------------+\n",
            "|  3 | P5           |              0.100 |\n",
            "+----+--------------+--------------------+\n",
            " Got 4 content-based recommendations\n",
            "\n",
            "PHASE 3: Combining recommendations with weights...\n",
            "Weights: Collaborative=0.6, Content=0.4\n",
            "              HYBRID RECOMMENDATIONS FOR USER 3             \n",
            "\n",
            "Top 10 Combined Recommendations:\n",
            "+----+--------------+------------------+\n",
            "|    | Product ID   |   Combined Score |\n",
            "+====+==============+==================+\n",
            "|  0 | 2            |            0.600 |\n",
            "+----+--------------+------------------+\n",
            "|  1 | 0            |            0.600 |\n",
            "+----+--------------+------------------+\n",
            "|  2 | P3           |            0.400 |\n",
            "+----+--------------+------------------+\n",
            "|  3 | P4           |            0.400 |\n",
            "+----+--------------+------------------+\n",
            "|  4 | P1           |            0.400 |\n",
            "+----+--------------+------------------+\n",
            "|  5 | P5           |            0.400 |\n",
            "+----+--------------+------------------+\n",
            "\n",
            "Total unique recommendations: 6\n",
            "              FINAL RECOMMENDATIONS FOR USER 3              \n",
            "\n",
            "Returned 6 recommendations:\n",
            "[2, 0, np.str_('P3'), np.str_('P4'), np.str_('P1'), np.str_('P5')]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 7.Export data"
      ],
      "metadata": {
        "id": "dP9nKuBVyEFI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import os\n",
        "\n",
        "def export_model_data_separately(model_data, directory=\"/content/drive/MyDrive/Recommendation Project/\"):\n",
        "    \"\"\"\n",
        "    Export trained model components into separate CSV files.\n",
        "\n",
        "    Args:\n",
        "        model_data (dict): Dictionary containing all trained model components.\n",
        "        directory (str): Path where CSV files will be saved.\n",
        "\n",
        "    Returns:\n",
        "        bool: True if export succeeded, False otherwise.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        # Ensure directory exists\n",
        "        os.makedirs(directory, exist_ok=True)\n",
        "        print(f\"\\nStarting export of model data to {directory}...\\n\")\n",
        "\n",
        "        # Validate required keys in model_data\n",
        "        required_keys = ['user_ids', 'product_ids', 'user_similarity', 'product_similarity', 'purchase_matrix', 'hybrid_weights']\n",
        "        for key in required_keys:\n",
        "            if key not in model_data:\n",
        "                raise KeyError(f\"Missing required model data key: {key}\")\n",
        "\n",
        "        # Prepare user similarity data\n",
        "        user_sim_df = pd.DataFrame(\n",
        "            model_data['user_similarity'],\n",
        "            columns=[f\"user_sim_{i}\" for i in range(model_data['user_similarity'].shape[1])],\n",
        "            index=model_data['user_ids']\n",
        "        ).reset_index().rename(columns={'index': 'user_id'})\n",
        "        user_sim_df.to_csv(os.path.join(directory, \"user_similarity.csv\"), index=False)\n",
        "        print(f\"User similarity matrix saved!\")\n",
        "\n",
        "        # Prepare product similarity data\n",
        "        product_sim_df = pd.DataFrame(\n",
        "            model_data['product_similarity'],\n",
        "            columns=[f\"product_sim_{i}\" for i in range(model_data['product_similarity'].shape[1])],\n",
        "            index=model_data['product_ids']\n",
        "        ).reset_index().rename(columns={'index': 'product_id'})\n",
        "        product_sim_df.to_csv(os.path.join(directory, \"product_similarity.csv\"), index=False)\n",
        "        print(f\"Product similarity matrix saved!\")\n",
        "\n",
        "        # Prepare purchase matrix\n",
        "        purchase_matrix = model_data['purchase_matrix']\n",
        "        purchase_matrix.index.name = 'user_id'\n",
        "        purchase_matrix.columns.name = 'product_id'\n",
        "        purchase_df = purchase_matrix.reset_index().melt(\n",
        "            id_vars='user_id',\n",
        "            var_name='product_id',\n",
        "            value_name='purchase_value'\n",
        "        )\n",
        "        purchase_df.to_csv(os.path.join(directory, \"purchase_data.csv\"), index=False)\n",
        "        print(f\"Purchase records saved!\")\n",
        "\n",
        "        # Prepare hybrid weights\n",
        "        weights_df = pd.DataFrame.from_dict(\n",
        "            model_data['hybrid_weights'],\n",
        "            orient='index',\n",
        "            columns=['weight_value']\n",
        "        ).reset_index().rename(columns={'index': 'weight_type'})\n",
        "        weights_df.to_csv(os.path.join(directory, \"hybrid_weights.csv\"), index=False)\n",
        "        print(f\"Model weights saved!\")\n",
        "\n",
        "        print(\"\\nSuccessfully exported all model data to separate files.\")\n",
        "        return True\n",
        "\n",
        "    except KeyError as ke:\n",
        "        print(f\"\\nKeyError: {str(ke)} - Please check model_data keys.\")\n",
        "        return False\n",
        "    except Exception as e:\n",
        "        print(f\"\\nError exporting model data: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# TESTING THE FUNCTION WITH SAMPLE DATA\n",
        "if __name__ == \"__main__\":\n",
        "    model_data = {\n",
        "        'user_ids': np.array(['U1', 'U2', 'U3']),\n",
        "        'product_ids': np.array(['P101', 'P102', 'P103', 'P104']),\n",
        "        'user_similarity': np.random.rand(3, 3),\n",
        "        'product_similarity': np.random.rand(4, 4),\n",
        "        'purchase_matrix': pd.DataFrame(\n",
        "            np.random.randint(0, 2, (3, 4)),\n",
        "            index=['U1', 'U2', 'U3'],\n",
        "            columns=['P101', 'P102', 'P103', 'P104']\n",
        "        ),\n",
        "        'hybrid_weights': {\n",
        "            'content_weight': 0.4,\n",
        "            'collab_weight': 0.3,\n",
        "            'popularity_weight': 0.2,\n",
        "            'recency_weight': 0.1\n",
        "        }\n",
        "    }\n",
        "\n",
        "    export_success = export_model_data_separately(model_data)\n",
        "\n",
        "    if export_success:\n",
        "        print(\"\\nAll trained data successfully exported to separate CSV files!\")\n",
        "    else:\n",
        "        print(\"\\nExport failed - check error messages.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K35YX6b2yKwm",
        "outputId": "7a0973f8-6e9c-4ce2-9613-5bdce66524d7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Starting export of model data to /content/drive/MyDrive/Recommendation Project/...\n",
            "\n",
            "User similarity matrix saved!\n",
            "Product similarity matrix saved!\n",
            "Purchase records saved!\n",
            "Model weights saved!\n",
            "\n",
            "Successfully exported all model data to separate files.\n",
            "\n",
            "All trained data successfully exported to separate CSV files!\n"
          ]
        }
      ]
    }
  ]
}